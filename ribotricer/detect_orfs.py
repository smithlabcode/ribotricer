"""Utilities for translating ORF detection"""

# Part of ribotricer software
#
# Copyright (C) 2020 Saket Choudhary, Wenzheng Li, and Andrew D Smith
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

from __future__ import annotations

import datetime
from collections import Counter, defaultdict
from typing import Final

import numpy as np
from quicksect import Interval, IntervalTree
from tqdm.autonotebook import tqdm

from .bam import AlignmentDict, ReadLengthCounts, split_bam
from .common import collapse_coverage_to_codon, mkdir_p, parent_dir
from .const import (
    CUTOFF,
    MINIMUM_DENSITY_OVER_ORF,
    MINIMUM_READS_PER_CODON,
    MINIMUM_VALID_CODONS,
    MINIMUM_VALID_CODONS_RATIO,
)
from .infer_protocol import infer_protocol
from .metagene import align_metagenes, metagene_coverage
from .orf import ORF
from .plotting import plot_metagene, plot_read_lengths
from .statistics import phasescore

tqdm.pandas()


# Required for IntervalTree
STRAND_TO_NUM: Final[dict[str, int]] = {"+": 1, "-": -1}

# Type aliases
MergedAlignments = defaultdict[str, Counter[tuple[str, int]]]
PsiteOffsets = dict[int, int]
RefSeq = defaultdict[str, IntervalTree]


def merge_read_lengths(
    alignments: AlignmentDict,
    psite_offsets: PsiteOffsets,
) -> MergedAlignments:
    """Merge read counts for different read lengths after applying appropriate offset(s).

    Parameters
    ----------
    alignments : AlignmentDict
        BAM split by length, strand.
    psite_offsets : PsiteOffsets
        Key is the length, value is the offset.

    Returns
    -------
    MergedAlignments
        Alignments by merging all lengths.
    """
    merged_alignments: MergedAlignments = defaultdict(Counter)

    for length, offset in list(psite_offsets.items()):
        for strand in alignments[length]:
            for chrom, pos in alignments[length][strand]:
                count = alignments[length][strand][(chrom, pos)]
                if strand == "+":
                    pos_shifted = pos + offset
                else:
                    pos_shifted = pos - offset
                merged_alignments[strand][(chrom, pos_shifted)] += count
    return merged_alignments


def parse_ribotricer_index(ribotricer_index: str) -> tuple[list[ORF], RefSeq]:
    """Parse ribotricer index to get only 'annotated' features.

    Parameters
    ----------
    ribotricer_index : str
        Path to the index file generated by ribotricer prepare_orfs.

    Returns
    -------
    tuple[list[ORF], RefSeq]
        Tuple of (annotated ORFs, refseq interval tree).
    """
    annotated: list[ORF] = []
    refseq: RefSeq = defaultdict(IntervalTree)

    # First count the number of
    # annotated regions to count.
    # The annotated regions appear first in the index file
    # so need to read only upto a point where the regions
    # no longer have the annotated tag.
    total_lines = 0
    with open(ribotricer_index, "r") as anno:
        # read header
        anno.readline()
        while "annotated" in anno.readline():
            total_lines += 1
    with open(ribotricer_index, "r") as anno:
        with tqdm(total=total_lines, unit="lines", leave=False) as pbar:
            # read header
            anno.readline()
            line = anno.readline()
            while "annotated" in line:
                pbar.update()
                orf = ORF.from_string(line)
                if orf is not None and orf.category == "annotated":
                    refseq[orf.chrom].insert(
                        Interval(
                            orf.intervals[0].start,
                            orf.intervals[-1].end,
                            STRAND_TO_NUM[orf.strand],
                        )
                    )
                    annotated.append(orf)
                line = anno.readline()
    return (annotated, refseq)


def orf_coverage(
    orf: ORF,
    alignments: MergedAlignments,
    offset_5p: int = 0,
    offset_3p: int = 0,
) -> list[int]:
    """Compute coverage for an ORF.

    Parameters
    ----------
    orf : ORF
        Instance of ORF.
    alignments : MergedAlignments
        Alignments summarized from BAM by merging lengths.
    offset_5p : int, optional
        The number of nts to include from 5'prime, by default 0.
    offset_3p : int, optional
        The number of nts to include from 3'prime, by default 0.

    Returns
    -------
    list[int]
        Coverage for ORF.
    """
    coverage: list[int] = []
    chrom = orf.chrom
    strand = orf.strand
    if strand == "-":
        offset_5p, offset_3p = offset_3p, offset_5p
    first, last = orf.intervals[0], orf.intervals[-1]
    for pos in range(first.start - offset_5p, first.start):
        if orf.category == "annotated":
            try:
                coverage.append(alignments[strand][(chrom, pos)])
            except KeyError:
                coverage.append(0)
        else:
            if strand in alignments and (chrom, pos) in alignments[strand]:
                coverage.append(alignments[strand][(chrom, pos)])
            else:
                coverage.append(0)

    for interval in orf.intervals:
        for pos in range(interval.start, interval.end + 1):
            if orf.category == "annotated":
                try:
                    coverage.append(alignments[strand][(chrom, pos)])
                except KeyError:
                    coverage.append(0)
            else:
                if strand in alignments and (chrom, pos) in alignments[strand]:
                    coverage.append(alignments[strand][(chrom, pos)])
                else:
                    coverage.append(0)

    for pos in range(last.end + 1, last.end + offset_3p + 1):
        if orf.category == "annotated":
            try:
                coverage.append(alignments[strand][(chrom, pos)])
            except KeyError:
                coverage.append(0)
        else:
            if strand in alignments and (chrom, pos) in alignments[strand]:
                coverage.append(alignments[strand][(chrom, pos)])
            else:
                coverage.append(0)

    if strand == "-":
        coverage.reverse()
    return coverage


def export_orf_coverages(
    ribotricer_index: str,
    merged_alignments: MergedAlignments,
    prefix: str,
    phase_score_cutoff: float = CUTOFF,
    min_valid_codons: int = MINIMUM_VALID_CODONS,
    min_reads_per_codon: float = MINIMUM_READS_PER_CODON,
    min_valid_codons_ratio: float = MINIMUM_VALID_CODONS_RATIO,
    min_density_over_orf: float = MINIMUM_DENSITY_OVER_ORF,
    report_all: bool = False,
) -> None:
    """Export ORF coverages to file.

    Parameters
    ----------
    ribotricer_index : str
        Path to the index file generated by ribotricer prepare_orfs.
    merged_alignments : MergedAlignments
        Alignments by merging all lengths.
    prefix : str
        Prefix for output file.
    phase_score_cutoff : float, optional
        Phase score cutoff value, by default CUTOFF.
    min_valid_codons : int, optional
        Minimum valid codons, by default MINIMUM_VALID_CODONS.
    min_reads_per_codon : float, optional
        Minimum reads per codon, by default MINIMUM_READS_PER_CODON.
    min_valid_codons_ratio : float, optional
        Minimum valid codons ratio, by default MINIMUM_VALID_CODONS_RATIO.
    min_density_over_orf : float, optional
        Minimum density over ORF, by default MINIMUM_DENSITY_OVER_ORF.
    report_all : bool, optional
        If True, all coverages will be exported, by default False.
    """
    # print('exporting coverages for all ORFs...')
    columns = [
        "ORF_ID",
        "ORF_type",
        "status",
        "phase_score",
        "read_count",
        "length",
        "valid_codons",
        "valid_codons_ratio",
        "read_density",
        "transcript_id",
        "transcript_type",
        "gene_id",
        "gene_name",
        "gene_type",
        "chrom",
        "strand",
        "start_codon",
        "profile\n",
    ]
    to_write = "\t".join(columns)
    formatter = "{}\t" * (len(columns) - 1) + "{}\n"
    with open(ribotricer_index, "r") as anno:
        total_lines = len(["" for line in anno])

    with (
        open(ribotricer_index, "r") as anno,
        open("{}_translating_ORFs.tsv".format(prefix), "w") as output,
    ):
        output.write(to_write)
        with tqdm(total=total_lines, unit="ORFs") as pbar:
            # Skip header
            anno.readline()
            for line in anno:
                pbar.update()
                orf = ORF.from_string(line)
                cov = orf_coverage(orf, merged_alignments)
                count = sum(cov)
                length = len(cov)
                coh, valid_codons = phasescore(cov)
                n_codons = max(1, length // 3)

                # codon level coverage
                codon_coverage = np.array(collapse_coverage_to_codon(cov))
                valid_codons_ratio = valid_codons / n_codons
                # total reads in the ORF divided by the length
                orf_density = np.sum(codon_coverage) / n_codons
                codon_coverage_exceeds_min = codon_coverage >= min_reads_per_codon
                status = (
                    "translating"
                    if (
                        coh >= phase_score_cutoff
                        and valid_codons >= min_valid_codons
                        and np.all(codon_coverage_exceeds_min)
                        and valid_codons_ratio >= min_valid_codons_ratio
                        and orf_density >= min_density_over_orf
                    )
                    else "nontranslating"
                )
                # skip outputing nontranslating ones
                if not report_all and status == "nontranslating":
                    pass
                else:
                    to_write = formatter.format(
                        orf.oid,
                        orf.category,
                        status,
                        coh,
                        count,
                        length,
                        valid_codons,
                        valid_codons_ratio,
                        orf_density,
                        orf.tid,
                        orf.ttype,
                        orf.gid,
                        orf.gname,
                        orf.gtype,
                        orf.chrom,
                        orf.strand,
                        orf.start_codon,
                        cov,
                    )
                    output.write(to_write)


def export_wig(merged_alignments: MergedAlignments, prefix: str) -> None:
    """Export merged alignments to WIG files.

    Parameters
    ----------
    merged_alignments : MergedAlignments
        Alignments by merging all lengths.
    prefix : str
        Prefix of output WIG files.
    """
    # print('exporting merged alignments to wig file...')
    for strand in merged_alignments:
        to_write = ""
        cur_chrom = ""
        for chrom, pos in sorted(merged_alignments[strand]):
            if chrom != cur_chrom:
                cur_chrom = chrom
                to_write += "variableStep chrom={}\n".format(chrom)
            to_write += "{}\t{}\n".format(pos, merged_alignments[strand][(chrom, pos)])
        if strand == "+":
            fname = "{}_pos.wig".format(prefix)
        else:
            fname = "{}_neg.wig".format(prefix)
        with open(fname, "w") as output:
            output.write(to_write)


def detect_orfs(
    bam: str,
    ribotricer_index: str,
    prefix: str,
    protocol: str | None,
    read_lengths: list[int] | None,
    psite_offsets: PsiteOffsets | None,
    phase_score_cutoff: float,
    min_valid_codons: int,
    min_reads_per_codon: float,
    min_valid_codons_ratio: float,
    min_density_over_orf: float,
    report_all: bool,
    meta_min_reads: int = 100000,
) -> None:
    """Detect translating ORFs from Ribo-seq data.

    Parameters
    ----------
    bam : str
        Path to the BAM file.
    ribotricer_index : str
        Path to the index file generated by ribotricer prepare_orfs.
    prefix : str
        Prefix for all output files.
    protocol : str | None
        Protocol: 'forward', 'no', or 'reverse'. If None, will be inferred.
    read_lengths : list[int] | None
        Read lengths to use. If None, will be automatically determined.
    psite_offsets : PsiteOffsets | None
        P-site offsets for each read length. If None, will be aligned using
        cross-correlation.
    phase_score_cutoff : float
        Phase score cutoff value for tagging an ORF as translating.
    min_valid_codons : int
        Minimum valid codons.
    min_reads_per_codon : float
        Minimum reads per codon.
    min_valid_codons_ratio : float
        Minimum valid codons ratio.
    min_density_over_orf : float
        Minimum density over ORF.
    report_all : bool
        Whether to output all ORFs' scores regardless of translation status.
    meta_min_reads : int, optional
        Minimum number of reads for a read length to be considered, by default 100000.
    """
    now = datetime.datetime.now()
    print(now.strftime("%b %d %H:%M:%S ..... started ribotricer detect-orfs"))

    # parse the index file
    now = datetime.datetime.now()
    print(now.strftime("%b %d %H:%M:%S ... started parsing ribotricer index file"))
    annotated, refseq = parse_ribotricer_index(ribotricer_index)

    # create directory
    mkdir_p(parent_dir(prefix))

    # infer experimental protocol if not provided
    if protocol is None:
        now = datetime.datetime.now()
        print(
            "{} ... {}".format(
                now.strftime("%b %d %H:%M:%S"),
                "started inferring experimental design",
            )
        )
        protocol = infer_protocol(bam, refseq, prefix)
    del refseq

    # split bam file into strand and read length
    now = datetime.datetime.now()
    print(now.strftime("%b %d %H:%M:%S ... started reading bam file"))
    alignments, read_length_counts = split_bam(bam, protocol, prefix, read_lengths)

    # plot read length distribution
    now = datetime.datetime.now()
    print(
        "{} ... {}".format(
            now.strftime("%b %d %H:%M:%S"),
            "started plotting read length distribution",
        )
    )
    plot_read_lengths(read_length_counts, prefix)

    # calculate metagene profiles
    now = datetime.datetime.now()
    print(
        "{} ... {}".format(
            now.strftime("%b %d %H:%M:%S"),
            "started calculating metagene profiles. This may take a long time...",
        )
    )
    metagenes = metagene_coverage(
        annotated,
        alignments,
        read_length_counts,
        prefix,
        meta_min_reads=meta_min_reads,
    )

    # plot metagene profiles
    now = datetime.datetime.now()
    print(
        "\n{} ... {}".format(
            now.strftime("%b %d %H:%M:%S"),
            "started plotting metagene profiles",
        )
    )
    plot_metagene(metagenes, read_length_counts, prefix)

    # align metagenes if psite_offsets not provided
    if psite_offsets is None:
        now = datetime.datetime.now()
        print(
            "{} ... {}".format(
                now.strftime("%b %d %H:%M:%S"),
                "started inferring P-site offsets",
            )
        )
        psite_offsets = align_metagenes(
            metagenes,
            read_length_counts,
            prefix,
            phase_score_cutoff,
            read_lengths is None,
        )

    # merge read lengths based on P-sites offsets
    now = datetime.datetime.now()
    print(
        "{} ... {}".format(
            now.strftime("%b %d %H:%M:%S"),
            "started shifting according to P-site offsets",
        )
    )
    merged_alignments = merge_read_lengths(alignments, psite_offsets)

    # export wig file
    now = datetime.datetime.now()
    print(
        "{} ... {}".format(
            now.strftime("%b %d %H:%M:%S"),
            "started exporting wig file of alignments after shifting",
        )
    )
    export_wig(merged_alignments, prefix)

    # saving detecting results to disk
    now = datetime.datetime.now()
    print(
        "{} ... {}".format(
            now.strftime("%b %d %H:%M:%S"),
            "started calculating phase scores for each ORF",
        )
    )
    export_orf_coverages(
        ribotricer_index,
        merged_alignments,
        prefix,
        phase_score_cutoff,
        min_valid_codons,
        min_reads_per_codon,
        min_valid_codons_ratio,
        min_density_over_orf,
        report_all,
    )
    now = datetime.datetime.now()
    print(
        "{} ... {}".format(
            now.strftime("%b %d %H:%M:%S"), "finished ribotricer detect-orfs"
        )
    )
