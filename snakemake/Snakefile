shell.executable("/bin/bash")
shell.prefix("source ~/.bashrc; ")

from collections import defaultdict
import os
import sys
import tempfile
from itertools import chain
from os.path import join
import glob
import re
import pandas as pd
import numpy as np
from snakemake.utils import report
from textwrap import dedent
import base64

TMP_DIR_ROOT = None
INTRON_BED = None

include:
    config['config_path']

workdir: OUT_DIR

if not TMP_DIR_ROOT:
    TMP_DIR_ROOT = '/tmp'

if not INTRON_BED:
    INTRON_BED = CDS_BED.replace('cds', 'intron')

def total_genome_size():
    df = pd.read_table(CHROM_SIZES, names=['chrom', 'sizes'])
    total = df['sizes'].sum()
    return total


def get_align_intro_params():
    df = pd.read_table(INTRON_BED, names=['chrom', 'start', 'end', 'name', 'score', 'strand'])
    lengths = df['end'] - df['start']

    ## Based on small genomes. See https://groups.google.com/forum/#!topic/rna-star/hQeHTBbkc0c
    alignintronNmin = max(4, lengths.min())
    alignintronNmax = lengths.max()
    return alignintronNmin, alignintronNmax

ALIGN_INTRON_Nmin, ALIGN_INTRON_Nmax = get_align_intro_params()

TOTAL_GENOME_SIZE = total_genome_size()
## Small genome optimization
## See STAR manual 2.2.5
SA_INDEX_Nbases = int(np.floor(min(14, np.log2(TOTAL_GENOME_SIZE)/2.0-1)))

ALL_SRA_FILES = glob.glob('{}/**/*.sra'.format(RAWDATA_DIR), recursive=True)
SRX_ID_DICT = defaultdict(list)
for sample in ALL_SRA_FILES:
    srx, srr = sample.replace('{}'.format(RAWDATA_DIR),'').lstrip('/').rstrip('/').split('/')
    SRX_ID_DICT[srx].append(srr.replace('.sra', ''))
#print(SRX_ID_DICT)

SRX_ID_DICT_VALUES = list(SRX_ID_DICT.values())
SRX_ID_DICT_KEYS = list(SRX_ID_DICT.keys())

ALL_SRR = [item for sublist in SRX_ID_DICT_VALUES for item in sublist]


def merge_bams_input(wildcards):
    return ['mapped/srr_bams/{}.bam'.format(srr) for srr in SRX_ID_DICT[wildcards.sample]]

def sra_to_fastq_input(wildcards):
    srr_id = wildcards.sample
    for key in list(SRX_ID_DICT.keys()):
        value = SRX_ID_DICT[key]
        if srr_id in list(value):
            srx_id = key
            return str(os.path.join(RAWDATA_DIR, srx_id, srr_id+'.sra'))
    print("WRONG encodeterend: {}".format(srr_id))
#print (ALL_SRR)

rule all:
    input:
        expand('sratofastq/{sample}.fastq.gz', sample=ALL_SRR),
        expand('preprocessed/{sample}_trimmed.fq.gz', sample=ALL_SRR),
        expand('mapped/bams/{sample}.bam', sample=SRX_ID_DICT_KEYS),
        expand('mapped/bigWigs/{sample}.bw', sample=SRX_ID_DICT_KEYS),
        expand('mapped/plots/metagene/{sample}.png', sample=SRX_ID_DICT_KEYS),
        expand('mapped/plots/read_length/{sample}.png', sample=SRX_ID_DICT_KEYS),
        expand('mapped/genewise_counts_CDS/{sample}.tsv', sample=SRX_ID_DICT_KEYS),
        'riboraptor_report.html'
        #expand('mapped/gene_coverages/CDS/{sample}_gene_coverages.tsv', sample=SRX_ID_DICT_KEYS),
        #expand('mapped/gene_coverages/UTR5/{sample}_gene_coverages.tsv', sample=SRX_ID_DICT_KEYS),
        #expand('mapped/gene_coverages/UTR3/{sample}_gene_coverages.tsv', sample=SRX_ID_DICT_KEYS),


rule sra_to_fastq:
    input: sra_to_fastq_input
    output: 'sratofastq/{sample}.fastq.gz'
    params:
        prefix='sratofastq/{sample}.fastq'
    shell:
        r'''fastq-dump --split-3 -O sratofastq {input} && gzip {params.prefix}
        '''


rule perfom_trimming:
    input:
        R1='sratofastq/{sample}.fastq.gz',
    params:
        out_dir='preprocessed/',
        phred_cutoff=5
    output:
        'preprocessed/{sample}_trimmed.fq.gz',
    shell:
        r'''
        trim_galore -o {params.out_dir} -q {params.phred_cutoff} {input.R1}
        '''


rule map_star:
    input:
        R1='preprocessed/{sample}_trimmed.fq.gz',
        index=STAR_INDEX
    output:
        bam='mapped/srr_bams/{sample}.bam',
        txbam='mapped/srr_tx_bams/{sample}.bam',
        counts='mapped/STARcounts/{sample}.counts',
    params:
        name = '{sample}',
        prefix = 'mapped/srr_bams/{sample}',
        unmapped = 'unmapped_fastq/{sample}',
        starlogs = 'mapped/starlogs',
    threads: 16
    run:
        with tempfile.TemporaryDirectory(dir=TMP_DIR_ROOT) as temp_dir:
            shell(r'''
                  STAR --runThreadN {threads}\
                       --genomeDir {input.index}\
                       --outFilterMismatchNmax 2\
                       --alignIntronMin {ALIGN_INTRON_Nmin}\
                       --alignIntronMax {ALIGN_INTRON_Nmax}\
                       --outFileNamePrefix {params.prefix}\
                       --readFilesIn {input.R1}\
                       --readFilesCommand zcat\
                       --quantMode TranscriptomeSAM GeneCounts\
                       --outSAMtype BAM Unsorted\
                       --outTmpDir {temp_dir}/{params.name}_tmp\
                       --outFilterType BySJout\
                       --outFilterMatchNmin 16\
                       && samtools sort -@ {threads} {params.prefix}Aligned.out.bam -o {output.bam} -T {temp_dir}/{params.name}_sort\
                       && mv {params.prefix}Aligned.toTranscriptome.out.bam {output.txbam}\
                       && samtools index {output.bam}\
                       && mv {params.prefix}ReadsPerGene.out.tab {output.counts}\
                       && mkdir -p {params.starlogs}\
                       && mv {params.prefix}Log.final.out {params.prefix}Log.out {params.prefix}SJ.out.tab\
                       {params.prefix}Log.progress.out {params.starlogs}\
                       && mkdir -p {params.unmapped}
                 ''')


rule merge_bams:
    input: merge_bams_input
    threads: 16
    output: 'mapped/bams/{sample}.bam'
    run:
        if len(input) > 1:
            with tempfile.TemporaryDirectory(dir=TMP_DIR_ROOT) as temp_dir:
                cmd = ' -in '.join(input)
                shell(r'''bamtools merge -in {cmd} -out {output}.unsorted \
                && samtools sort -@ {threads} -T {temp_dir}/{wildcards.sample}_merge_bam -o {output} {output}.unsorted \
                && samtools index {output} \
                && yes | rm -rf {output}.unsorted''')
        elif len(input) == 1:
            cmd = input[0]
            shell('''mv {cmd} {output} \
            && mv {cmd}.bai {output}.bai''')


rule create_uniq_bedgraph_from_bam:
    input: 'mapped/bams_unique/{sample}.bam'
    output: 'mapped/bedGraphs/{sample}.bg'
    params: 
    shell:
        r'''
        riboraptor bam-to-bedgraph --bam {input} \
        --end_type 5prime \
        --strand + \
        --saveto {output} \
        && bedSort {output} {output}
        '''

rule create_uniq_bigwig_from_uniq_bedgraph_raw:
    input: 'mapped/bedGraphs/{sample}.bg',
    output: 'mapped/bigWigs/{sample}.bw',
    shell:
        r'''bedGraphToBigWig {input} {CHROM_SIZES} {output}'''


rule export_utr5_coverage:
    input: 'mapped/bigWigs/{sample}.bw'
    params:
        offset_5p = 10
    output: 'mapped/gene_coverages/UTR5/{sample}_gene_coverages.tsv'
    shell: r'''riboraptor export-gene-coverages \
           --bigwig {input} \
           --region_bed {UTR5_BED} \
           --saveto {output} \
           --offset_5p {params.offset_5p}'''


rule export_cds_coverage:
    input: 'mapped/bigWigs/{sample}.bw'
    params:
        offset_5p = 10
    output: 'mapped/gene_coverages/CDS/{sample}_gene_coverages.tsv'
    shell: r'''riboraptor export-gene-coverages \
           --bigwig {input} \
           --region_bed {CDS_BED} \
           --saveto {output} \
           --offset_5p {params.offset_5p}'''


rule export_utr3_coverage:
    input: 'mapped/bigWigs/{sample}.bw'
    params:
        offset_5p = 10
    output: 'mapped/gene_coverages/UTR3/{sample}_gene_coverages.tsv'
    shell: r'''riboraptor export-gene-coverages \
           --bigwig {input} \
           --region_bed {UTR3_BED} \
           --saveto {output} \
           --offset_5p {params.offset_5p}'''


rule extract_uniq_mapping:
    input: 'mapped/bams/{sample}.bam'
    output: 'mapped/bams_unique/{sample}.bam'
    threads: 16
    run:
        with tempfile.TemporaryDirectory(dir=TMP_DIR_ROOT) as temp_dir:
            shell(r'''
            samtools view -b -q 255 {input} -o {output}.temp \
            && samtools sort -@ {threads} {output}.temp -o {output} -T {temp_dir}/{wildcards.sample}_sort \
            && rm -rf {output}.temp \
            && samtools index {output}
            ''')

rule fragment_length_pickle:
    input: 'mapped/bams_unique/{sample}.bam'
    output: 'mapped/fragment_length_pickle/{sample}.tsv'
    shell:
        r'''riboraptor read-length-dist --bam {input} --saveto {output}'''


rule export_read_length:
    input: 'mapped/bams_unique/{sample}.bam'
    output: 'mapped/read_lengths/{sample}.tsv'
    shell: 
        r'''
        riboraptor export-read-length --bam {input} --saveto {output}
        '''

rule plot_read_length:
    input: 'mapped/read_lengths/{sample}.tsv'
    output: 'mapped/plots/read_length/{sample}.png'
    shell:
        r'''
        riboraptor plot-read-length --millify_labels --read-lengths {input} --saveto {output}
        '''

rule export_metagene:
    input: 'mapped/bigWigs/{sample}.bw'
    output: 'mapped/metagene_coverages/{sample}.tsv'
    shell:
        r'''
        riboraptor export-metagene-coverage --bigwig {input} \
        --region_bed {CDS_BED} --ignore_tx_version --saveto {output}
        '''

rule plot_metagene:
    input: 'mapped/metagene_coverages/{sample}.tsv'
    output: 'mapped/plots/metagene/{sample}.png'
    shell: 
        r'''
        riboraptor plot-metagene --counts {input} --saveto {output} --positions -60:300 --xrotation 90
        '''

rule metagene_coverage_cds2:
    input: 'mapped/bams_unique/{sample}.bam'
    output: 'mapped/genewise_counts_CDS/{sample}.tsv'
    shell:
        r'''riboraptor count-reads-bed --bam {input} --bed {CDS_BED} --saveto {output}
        '''

rule report:
    input:
      metagene = expand('mapped/plots/metagene/{sample}.png', sample=SRX_ID_DICT_KEYS),
      fragment_length = expand('mapped/plots/read_length/{sample}.png', sample=SRX_ID_DICT_KEYS),
    output:
        html='riboraptor_report.html'
    run:
        text = dedent("""
        <html>
        <center>
        <h1> Fragment Length Distribution </h1>


        """)

        for m in input.fragment_length:
            with open(m, 'rb') as f:
                encoded = base64.b64encode(f.read())
            img_tag = '<img alt="" src="data:image/png;base64,{0}" height="400px">'.format(str((encoded), 'utf-8'))

            text += dedent('''
            <figure>
            {img_tag}
            <figcaption> {fig_name} </figcaption>
            </figure>''').format(img_tag=img_tag, fig_name=m.split('/')[-1].replace('.png', ''))

        text += dedent('''

        <h1> Metagene Plots </h1>

                ''')

        for m in input.metagene:
            with open(m, 'rb') as f:
                encoded = base64.b64encode(f.read())
            img_tag = '<img alt="" src="data:image/png;base64,{0}" height="400px">'.format(str((encoded), 'utf-8'))

            text += dedent('''
            <figure>
            {img_tag}
            <figcaption> {fig_name} </figcaption>
            </figure>''').format(img_tag=img_tag, fig_name=m.split('/')[-1].replace('.png', ''))

        text += dedent("""
        </center>
        </html>
        """)
        #report(text=text, path=output.html, **input)
        with open(output.html, 'w') as f:
            f.write(text)

